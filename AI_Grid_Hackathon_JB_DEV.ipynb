{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "frfor6Q_gryv",
        "outputId": "8699026f-9805-4689-d823-9d570b422a95"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def train_test_split():\n",
        "    lp_20ips_2016 = pd.read_csv(\"LoadProfile_20IPs_2016.csv\", sep=\";\", skiprows=1)\n",
        "    lp_30ips_2017 = pd.read_csv(\"LoadProfile_30IPs_2017.csv\", sep=\";\", skiprows=1)\n",
        "\n",
        "    lp_20ips_2016[\"Time stamp\"] = lp_20ips_2016[\"Time stamp\"].str.replace(r'[^0-9.: ]', '', regex=True).str.strip()\n",
        "    lp_20ips_2016[\"Time stamp\"] = pd.to_datetime(lp_20ips_2016.loc[:, \"Time stamp\"], format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "    lp_30ips_2017[\"Time stamp\"] = lp_30ips_2017[\"Time stamp\"].str.replace(r'[^0-9.: ]', '', regex=True).str.strip()\n",
        "    lp_30ips_2017[\"Time stamp\"] = pd.to_datetime(lp_30ips_2017.loc[:, \"Time stamp\"], format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "    train_2016_full = lp_20ips_2016.iloc[:, :-7]\n",
        "    train_2017_full = lp_30ips_2017.iloc[:, :-8]\n",
        "    test_2016_full = pd.concat([lp_20ips_2016[\"Time stamp\"], lp_20ips_2016.iloc[:, -7:] ], axis=1)\n",
        "    test_2017_full = pd.concat([lp_30ips_2017[\"Time stamp\"], lp_30ips_2017.iloc[:, -8:] ], axis=1)\n",
        "\n",
        "    train_2016_training_data = train_2016_full[train_2016_full[\"Time stamp\"].dt.month <= 8]\n",
        "    train_2016_val_data = train_2016_full[train_2016_full[\"Time stamp\"].dt.month > 8]\n",
        "\n",
        "    test_2016_training_data = test_2016_full[test_2016_full[\"Time stamp\"].dt.month <= 8]\n",
        "    test_2016_test_data = test_2016_full[test_2016_full[\"Time stamp\"].dt.month > 8]\n",
        "\n",
        "    train_2017_training_data = train_2017_full[train_2017_full[\"Time stamp\"].dt.month <= 8]\n",
        "    train_2017_val_data = train_2017_full[train_2017_full[\"Time stamp\"].dt.month > 8]\n",
        "\n",
        "    test_2017_training_data = test_2017_full[test_2017_full[\"Time stamp\"].dt.month <= 8]\n",
        "    test_2017_test_data = test_2017_full[test_2017_full[\"Time stamp\"].dt.month > 8]\n",
        "\n",
        "    train_2016_training_data.to_csv(\"tune/2016_train.csv\", index=False)\n",
        "    train_2016_val_data.to_csv(\"tune/2016_val.csv\", index=False)\n",
        "    test_2016_training_data.to_csv(\"test/2016_train.csv\", index=False)\n",
        "    test_2016_test_data.to_csv(\"test/2016_test.csv\", index=False)\n",
        "\n",
        "    train_2017_training_data.to_csv(\"tune/2017_train.csv\", index=False)\n",
        "    train_2017_val_data.to_csv(\"tune/2017_val.csv\", index=False)\n",
        "    test_2017_training_data.to_csv(\"test/2017_train.csv\", index=False)\n",
        "    test_2017_test_data.to_csv(\"test/2017_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_test_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Encoder\n",
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Define LSTM\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        \n",
        "        # Pass input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        # hn and cn will be passed to the decoder\n",
        "        return output, hn, cn\n",
        "\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, num_layers):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        \n",
        "        # Fully connected layer to output real values\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x, hidden, cell):\n",
        "        # Pass input through the LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (hidden, cell))\n",
        "        \n",
        "        # Pass through the fully connected layer to get real-valued predictions\n",
        "        output = self.fc(output)\n",
        "        return output, hn, cn\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        target_dim = target.size(2)\n",
        "        \n",
        "        # Tensor to store decoder outputs (real-valued)\n",
        "        outputs = torch.zeros(batch_size, target_len, target_dim).to(self.device)\n",
        "        \n",
        "        # Encode the source sequence\n",
        "        encoder_output, hidden, cell = self.encoder(source)\n",
        "        \n",
        "        # First input to the decoder is the first time step of the target\n",
        "        decoder_input = target[:, 0, :].unsqueeze(1)  # (batch_size, 1, output_dim)\n",
        "        \n",
        "        for t in range(1, target_len):\n",
        "            # Pass through the decoder\n",
        "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "            \n",
        "            # Store the output (real-valued prediction)\n",
        "            outputs[:, t, :] = output.squeeze(1)\n",
        "            \n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            decoder_input = target[:, t, :].unsqueeze(1) if teacher_force else output\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "def combined_loss(y_true_load, y_pred_load, y_true_outlier, alpha: float = 10.):\n",
        "    mse_loss = nn.MSELoss()(y_pred_load, y_true_load)\n",
        "    outlier_mse_loss = nn.MSELoss()(y_pred_load * y_true_outlier, y_true_load * y_true_outlier)\n",
        "    \n",
        "    total_loss = mse_loss + alpha * outlier_mse_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "RUrvBIX3hvmi",
        "outputId": "412cc536-cea4-4b7d-fda7-37d0c4df5e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating train dataset...\n",
            "Done.\n",
            "Creating val dataset...\n",
            "Done.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[106], line 294\u001b[0m\n\u001b[1;32m    284\u001b[0m forecaster \u001b[38;5;241m=\u001b[39m TorchForecaster(\n\u001b[1;32m    285\u001b[0m     year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2017\u001b[39m,\n\u001b[1;32m    286\u001b[0m     ig\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     forecast_horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m\n\u001b[1;32m    292\u001b[0m )\n\u001b[1;32m    293\u001b[0m forecaster\u001b[38;5;241m.\u001b[39mpreprocess()\n\u001b[0;32m--> 294\u001b[0m \u001b[43mforecaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m forecaster\u001b[38;5;241m.\u001b[39mvalidate()\n",
            "Cell \u001b[0;32mIn[106], line 196\u001b[0m, in \u001b[0;36mTorchForecaster.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     y_pred_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_window, y_window_load)\n\u001b[1;32m    191\u001b[0m     loss \u001b[38;5;241m=\u001b[39m combined_loss(\n\u001b[1;32m    192\u001b[0m         y_true_load\u001b[38;5;241m=\u001b[39my_window_load,\n\u001b[1;32m    193\u001b[0m         y_pred_load\u001b[38;5;241m=\u001b[39my_pred_window,\n\u001b[1;32m    194\u001b[0m         y_true_outlier\u001b[38;5;241m=\u001b[39my_window_outlier\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    198\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
        "from sktime.split import SlidingWindowSplitter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "class Forecaster(ABC):\n",
        "    def __init__(self, year: int, ig: int, lookback:int, forecast_horizon: int):\n",
        "        self.year = year\n",
        "        self.ig = ig\n",
        "        self.dataset, self.train_idx, self.val_idx = self.load_data(year, ig)\n",
        "        self.lookback = lookback\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "    @staticmethod\n",
        "    def load_data(year: int, ig: int) -> tuple[pd.DataFrame, pd.Index, pd.Index]:\n",
        "        if year == 2017:\n",
        "            ig_str = f\"LG {ig:02d}\"\n",
        "        else:\n",
        "            ig_str = f\"LG {ig:01d}\"\n",
        "\n",
        "        train = pd.read_csv(f\"tune/{year}_train.csv\")\n",
        "        val = pd.read_csv(f\"tune/{year}_val.csv\")\n",
        "        dataset = pd.concat([train, val], axis=0)\n",
        "\n",
        "        dataset = dataset[[\"Time stamp\", ig_str]]\n",
        "        dataset[ig_str] = dataset[ig_str].astype(float)\n",
        "        dataset.columns = [\"Time stamp\", \"target\"]\n",
        "        dataset[\"Time stamp\"] = pd.to_datetime(dataset.loc[:, \"Time stamp\"], errors='coerce')\n",
        "\n",
        "        train_idx = dataset[dataset[\"Time stamp\"].dt.month <= 8].index\n",
        "        val_idx = dataset[dataset[\"Time stamp\"].dt.month > 8].index\n",
        "\n",
        "        return dataset, train_idx, val_idx\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_public_holidays(year: int) -> list[datetime]:\n",
        "        with open(f\"holidays_{year}.json\", 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract unique dates and convert them to datetime objects\n",
        "        holiday_dates = set()\n",
        "        for state in data:\n",
        "            for holiday_info in data[state].values():\n",
        "                date_str = holiday_info[\"datum\"]\n",
        "                # holiday_dates.add(datetime.strptime(date_str, \"%Y-%m-%d\"))\n",
        "                holiday_dates.add(datetime.strptime(date_str, \"%Y-%m-%d\").date())\n",
        "\n",
        "        return list(holiday_dates)\n",
        "\n",
        "    @abstractmethod\n",
        "    def preprocess(self):\n",
        "        raise\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self, y_target):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def validate(self):\n",
        "        pass\n",
        "\n",
        "class TorchForecaster(Forecaster):\n",
        "    def __init__(\n",
        "            self,\n",
        "            year: int,\n",
        "            ig: int,\n",
        "            model: nn.Module,\n",
        "            loss_fn: nn.Module,\n",
        "            learning_rate: float = 0.001,\n",
        "            batch_size: int = 32,\n",
        "            peak_threshold: float = 0.85,\n",
        "            lookback: int = 672,\n",
        "            forecast_horizon: int = 48\n",
        "        ):\n",
        "        super().__init__(year, ig, lookback, forecast_horizon)\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.peak_threshold = peak_threshold\n",
        "\n",
        "        self.mean = self.dataset.loc[self.train_idx, \"target\"].mean()\n",
        "        self.std = self.dataset.loc[self.train_idx, \"target\"].std()\n",
        "        self.peak_value = self.dataset.loc[self.train_idx, \"target\"].max() * self.peak_threshold\n",
        "        self.normalized_peak_vaue = (self.peak_value - self.mean) / self.std\n",
        "\n",
        "        self.train_splitter = SlidingWindowSplitter(\n",
        "            fh=range(self.forecast_horizon), \n",
        "            window_length=self.lookback,\n",
        "            step_length=1\n",
        "        )\n",
        "\n",
        "        self.val_splitter = SlidingWindowSplitter(\n",
        "            fh=range(self.forecast_horizon), \n",
        "            window_length=self.lookback,\n",
        "            step_length=4 * 25\n",
        "        )\n",
        "\n",
        "        self.holidays = self.get_public_holidays(self.year)\n",
        "\n",
        "        self.training_process = []\n",
        "\n",
        "    def augment(self, train_data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Augment the training data with additional features.\"\"\"\n",
        "\n",
        "        train_data.loc[:, \"target_normalized\"] = (train_data.loc[:, \"target\"] - self.mean) / self.std\n",
        "        train_data.loc[:, \"is_peak\"] = (train_data.loc[:, \"target_normalized\"] >= self.normalized_peak_vaue).astype(int)\n",
        "\n",
        "        train_data.loc[:, \"dow\"] = train_data[\"Time stamp\"].dt.dayofweek\n",
        "        train_data.loc[:, \"hour\"] = train_data[\"Time stamp\"].dt.hour\n",
        "\n",
        "        train_data.loc[:, \"is_holiday\"] = train_data[\"Time stamp\"].dt.date.isin(self.holidays).astype(int)\n",
        "\n",
        "        return train_data\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"Data cleaning and normalization\"\"\"\n",
        "        self.dataset.loc[:, \"target\"] = self.dataset.loc[:, \"target\"].interpolate()\n",
        "        self.dataset = self.dataset.dropna(subset=[\"target\"])\n",
        "\n",
        "        self.dataset = self.augment(self.dataset)\n",
        "\n",
        "    def get_dataset(self, split: str = \"train\") -> TensorDataset:\n",
        "        print(f\"Creating {split} dataset...\")\n",
        "        if split == \"train\":\n",
        "            data = self.dataset[self.dataset[\"Time stamp\"].dt.month <= 8]\n",
        "            splitter = self.train_splitter\n",
        "        else:\n",
        "            data_train = self.dataset.iloc[-self.lookback:, :]\n",
        "            data_val = self.dataset.loc[self.dataset[\"Time stamp\"].dt.month > 8, :]\n",
        "            data = pd.concat([data_train, data_val], axis=0, ignore_index=True)\n",
        "            splitter = self.val_splitter\n",
        "\n",
        "        X_train_windows = []\n",
        "        y_train_windows = []\n",
        "\n",
        "        X_data = data[[\"target_normalized\", \"dow\", \"hour\", \"is_peak\", \"is_holiday\"]].values\n",
        "        y_data = data[[\"target_normalized\", \"is_peak\"]].values\n",
        "\n",
        "        for X_idx, y_idx in splitter.split(data[\"target_normalized\"]):\n",
        "            X_train, y_train = X_data[X_idx[:-1]], y_data[y_idx[1:]]\n",
        "\n",
        "            X_train_windows.append(X_train)\n",
        "            y_train_windows.append(y_train)\n",
        "\n",
        "        X_train_tensor = torch.tensor(X_train_windows, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_windows, dtype=torch.float32)\n",
        "\n",
        "        dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def train(self, epochs: int):\n",
        "        \"\"\"Fit the internal model(s)\"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        train_dataset = self.get_dataset(split=\"train\")\n",
        "        val_dataset = self.get_dataset(split=\"val\")\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        \n",
        "        best_model_state_dict = None\n",
        "        best_loss = np.inf\n",
        "\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            for i, (x_window, y_window) in enumerate(train_loader):\n",
        "\n",
        "                y_window_load = y_window[:, 0].unsqueeze(-1)\n",
        "                y_window_outlier = y_window[:, 1].unsqueeze(-1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                y_pred_window = self.model(x_window, y_window_load)\n",
        "                loss = combined_loss(\n",
        "                    y_true_load=y_window_load,\n",
        "                    y_pred_load=y_pred_window,\n",
        "                    y_true_outlier=y_window_outlier\n",
        "                )\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                print(f\"Batch {i + 1} - Loss: {loss.item():.2f}\")\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for x_window, y_window in train_loader:\n",
        "                    y_window_load = y_window[:, 0]\n",
        "                    y_window_outlier = y_window[:, 1]\n",
        "\n",
        "                    y_pred_window = self.model(x_window)\n",
        "                    loss = combined_loss(\n",
        "                        y_true_load=y_window_load,\n",
        "                        y_pred_load=y_pred_window,\n",
        "                        y_true_outlier=y_window_outlier\n",
        "                    )\n",
        "                    val_loss += loss.item()\n",
        "            val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                best_model_state_dict = self.model.state_dict()\n",
        "\n",
        "            print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}\")\n",
        "            self.training_process +=[{\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_loss\": val_loss\n",
        "            }]\n",
        "\n",
        "            assert best_model_state_dict is not None\n",
        "            self.model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "        pd.DataFrame(self.training_process).to_csv(f\"training_process_{self.year}_{self.ig}.csv\", index=False)\n",
        "            \n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Predict the next value(s)\"\"\"\n",
        "        # TODO calculate timeframe window\n",
        "        assert X.columns == [\"Time stamp\", \"target\"]\n",
        "\n",
        "        X = self.augment(X)\n",
        "        return self.model(X)\n",
        "\n",
        "    def update(self, y_target):\n",
        "        \"\"\"Update the model with the new value (if required)\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model on the validation data\"\"\"\n",
        "        errors = []\n",
        "\n",
        "        # iterate over validation target using sliding window\n",
        "        for i_start in range(0, len(self.val_data), self.fh):\n",
        "            y_target = self.val_data.loc[i_start : i_start + self.fh - 1, \"target\"]\n",
        "\n",
        "\n",
        "            if len(y_target) < self.fh:\n",
        "                break\n",
        "\n",
        "            y_hat = self.predict()\n",
        "\n",
        "            # de-normalize\n",
        "            y_hat_denormalized = y_hat * self.std + self.mean\n",
        "            \n",
        "            # TODO plug in actual validation error metric\n",
        "            error = mean_absolute_percentage_error(y_target, y_hat_denormalized)\n",
        "\n",
        "            errors.append(error)\n",
        "\n",
        "            self.update(y_target)\n",
        "        \n",
        "        return np.mean(errors)\n",
        "\n",
        "input_dim = 5\n",
        "output_dim = 1  # For example, size of the output features (same as input for many-to-many)\n",
        "hidden_dim = 64  # LSTM hidden dimension\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = EncoderLSTM(input_dim, hidden_dim, num_layers).to(device)\n",
        "decoder = DecoderLSTM(output_dim, hidden_dim, num_layers).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "\n",
        "forecaster = TorchForecaster(\n",
        "    year=2017,\n",
        "    ig=1,\n",
        "    model=model,\n",
        "    learning_rate=learning_rate,\n",
        "    loss_fn=combined_loss,\n",
        "    lookback=672,\n",
        "    forecast_horizon=48\n",
        ")\n",
        "forecaster.preprocess()\n",
        "forecaster.train(2)\n",
        "forecaster.validate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "AVAILABLE_DATASETS = {\n",
        "    2017: range(1, 23),\n",
        "    2016: range(1, 14),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(errors)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mevaluate_forecaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFORECASTER_CLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFORECASTER_KWARS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mevaluate_forecaster\u001b[0;34m(forecaster_cls, forecaster_kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m forecaster\u001b[38;5;241m.\u001b[39mpreprocess()\n\u001b[1;32m     17\u001b[0m forecaster\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mforecaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m errors\u001b[38;5;241m.\u001b[39mappend(error)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m   \u001b[38;5;66;03m# DEBUG\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[18], line 100\u001b[0m, in \u001b[0;36mSimpleForecaster.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_target) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfh:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# de-normalize\u001b[39;00m\n\u001b[1;32m    103\u001b[0m y_hat_denormalized \u001b[38;5;241m=\u001b[39m y_hat \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean\n",
            "Cell \u001b[0;32mIn[18], line 84\u001b[0m, in \u001b[0;36mSimpleForecaster.predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict the next value(s)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msk_fh\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/sktime/forecasting/base/_base.py:451\u001b[0m, in \u001b[0;36mBaseForecaster.predict\u001b[0;34m(self, fh, X)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# we call the ordinary _predict if no looping/vectorization needed\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_vectorized:\n\u001b[0;32m--> 451\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# otherwise we call the vectorized version of predict\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m=\u001b[39mX_inner, fh\u001b[38;5;241m=\u001b[39mfh)\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/sktime/forecasting/theta.py:174\u001b[0m, in \u001b[0;36mThetaForecaster._predict\u001b[0;34m(self, fh, X)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, fh, X):\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make forecasts.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m        Returns series of predicted values.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# Add drift.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     drift \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_drift()\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/sktime/forecasting/base/adapters/_statsmodels.py:124\u001b[0m, in \u001b[0;36m_StatsModelsAdapter._predict\u001b[0;34m(self, fh, X)\u001b[0m\n\u001b[1;32m    122\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fitted_forecaster\u001b[38;5;241m.\u001b[39mpredict(start\u001b[38;5;241m=\u001b[39mstart, end\u001b[38;5;241m=\u001b[39mend, exog\u001b[38;5;241m=\u001b[39mX)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fitted_forecaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# statsmodels forecasts all periods from start to end of forecasting\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# horizon, but only return given time points in forecasting horizon\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# if fh[0] > 1 steps ahead of cutoff then make relative to `start`\u001b[39;00m\n\u001b[1;32m    129\u001b[0m fh_int \u001b[38;5;241m=\u001b[39m fh_int \u001b[38;5;241m-\u001b[39m fh_int[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/statsmodels/base/wrapper.py:113\u001b[0m, in \u001b[0;36mmake_wrapper.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     obj \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mwrap_output(func(results, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), how[\u001b[38;5;241m0\u001b[39m], how[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how:\n\u001b[0;32m--> 113\u001b[0m     obj \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mwrap_output(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, how)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/results.py:251\u001b[0m, in \u001b[0;36mHoltWintersResults.predict\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    In-sample prediction and out-of-sample forecasting\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        Array of out of sample forecasts.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:513\u001b[0m, in \u001b[0;36mExponentialSmoothing.predict\u001b[0;34m(self, params, start, end)\u001b[0m\n\u001b[1;32m    511\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(h\u001b[38;5;241m=\u001b[39mout_of_sample, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mfittedfcast[start : end \u001b[38;5;241m+\u001b[39m out_of_sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/pandas/util/_decorators.py:213\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    212\u001b[0m     kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/pandas/util/_decorators.py:213\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    212\u001b[0m     kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:1391\u001b[0m, in \u001b[0;36mExponentialSmoothing._predict\u001b[0;34m(self, h, smoothing_level, smoothing_trend, smoothing_seasonal, initial_level, initial_trend, damping_trend, initial_seasons, use_boxcox, lamda, remove_bias, is_optimized)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nobs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1390\u001b[0m         lvls[i] \u001b[38;5;241m=\u001b[39m y_alpha[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1391\u001b[0m             alphac \u001b[38;5;241m*\u001b[39m trended(lvls[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[43mdampen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1392\u001b[0m         )\n\u001b[1;32m   1393\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_trend:\n\u001b[1;32m   1394\u001b[0m             b[i] \u001b[38;5;241m=\u001b[39m (beta \u001b[38;5;241m*\u001b[39m detrend(lvls[i], lvls[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1395\u001b[0m                 betac \u001b[38;5;241m*\u001b[39m dampen(b[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], phi)\n\u001b[1;32m   1396\u001b[0m             )\n",
            "File \u001b[0;32m~/anaconda3/envs/aigrid/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:1334\u001b[0m, in \u001b[0;36mExponentialSmoothing._predict.<locals>.<lambda>\u001b[0;34m(b, phi)\u001b[0m\n\u001b[1;32m   1328\u001b[0m trended \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmultiply, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mlambda\u001b[39;00m l, b: l}[\n\u001b[1;32m   1329\u001b[0m     trend\n\u001b[1;32m   1330\u001b[0m ]\n\u001b[1;32m   1331\u001b[0m detrend \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mdivide, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39msubtract, \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mlambda\u001b[39;00m l, b: \u001b[38;5;241m0\u001b[39m}[\n\u001b[1;32m   1332\u001b[0m     trend\n\u001b[1;32m   1333\u001b[0m ]\n\u001b[0;32m-> 1334\u001b[0m dampen \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mpower, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmultiply, \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mlambda\u001b[39;00m b, phi: \u001b[38;5;241m0\u001b[39m}[\n\u001b[1;32m   1335\u001b[0m     trend\n\u001b[1;32m   1336\u001b[0m ]\n\u001b[1;32m   1337\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnobs\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seasonal \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "FORECASTER_CLS = SimpleForecaster\n",
        "FORECASTER_KWARS = {\n",
        "    \"fh\": 48\n",
        "}\n",
        "\n",
        "def evaluate_forecaster(forecaster_cls, forecaster_kwargs):\n",
        "    errors = []\n",
        "\n",
        "    for year, igs in AVAILABLE_DATASETS.items():\n",
        "        for ig in igs:\n",
        "            forecaster = forecaster_cls(\n",
        "                year=year,\n",
        "                ig=ig,\n",
        "                **forecaster_kwargs\n",
        "            )\n",
        "            forecaster.preprocess()\n",
        "            forecaster.train()\n",
        "            error = forecaster.validate()\n",
        "            errors.append(error)\n",
        "            break   # DEBUG\n",
        "        break   # DEBUG\n",
        "\n",
        "    return np.mean(errors)\n",
        "\n",
        "evaluate_forecaster(FORECASTER_CLS, FORECASTER_KWARS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
