{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Encoder\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define LSTM\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Pass input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # hn and cn will be passed to the decoder\n",
    "        return output, hn, cn\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to output real values\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Pass input through the LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (hidden, cell))\n",
    "        \n",
    "        # Pass through the fully connected layer to get real-valued predictions\n",
    "        output = self.fc(output)\n",
    "        return output, hn, cn\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        target_len = target.size(1)\n",
    "        target_dim = target.size(2)\n",
    "        \n",
    "        # Tensor to store decoder outputs (real-valued)\n",
    "        outputs = torch.zeros(batch_size, target_len - 1, target_dim).to(self.device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        encoder_output, hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # First input to the decoder is the first time step of the target\n",
    "        decoder_input = target[:, 0, :].unsqueeze(1)  # (batch_size, 1, output_dim)\n",
    "        \n",
    "        for t in range(0, target_len - 1):\n",
    "            # Pass through the decoder\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            \n",
    "            # Store the output (real-valued prediction)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            decoder_input = target[:, t, :].unsqueeze(1) if teacher_force else output\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "def combined_loss(y_true_load, y_pred_load, y_true_outlier, alpha: float = 10.):\n",
    "    mse_loss = nn.MSELoss()(y_pred_load, y_true_load)\n",
    "    outlier_mse_loss = nn.MSELoss()(y_pred_load * y_true_outlier, y_true_load * y_true_outlier)\n",
    "    \n",
    "    total_loss = mse_loss + alpha * outlier_mse_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "RUrvBIX3hvmi",
    "outputId": "412cc536-cea4-4b7d-fda7-37d0c4df5e9e"
   },
   "outputs": [
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_239512/3692824928.py\", line 1, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/__init__.py\", line 1694, in <module>\n    _C._initExtension(_manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 259, in <module>\n    _lazy_call(_check_capability)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 256, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py:327\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py:195\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 195\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py:451\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py:469\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 296\u001b[0m\n\u001b[1;32m    293\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m    294\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 296\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderLSTM(output_dim, hidden_dim, num_layers)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, decoder, device)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/nn/modules/rnn.py:223\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 223\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py:333\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    329\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    330\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(orig_traceback)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             )\n\u001b[0;32m--> 333\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":49, please report a bug to PyTorch. device=\u0001, num_gpus=\u0001\n\nCUDA call was originally invoked at:\n\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_239512/3692824928.py\", line 1, in <module>\n    import torch\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/__init__.py\", line 1694, in <module>\n    _C._initExtension(_manager_path())\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 259, in <module>\n    _lazy_call(_check_capability)\n  File \"/bigwork/nhwpbecj/.conda/envs/24cast/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 256, in _lazy_call\n    _queued_calls.append((callable, traceback.format_stack()))\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "from sktime.split import SlidingWindowSplitter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "class Forecaster(ABC):\n",
    "    def __init__(self, year: int, ig: int, lookback:int, forecast_horizon: int):\n",
    "        self.year = year\n",
    "        self.ig = ig\n",
    "        self.dataset, self.train_idx, self.val_idx = self.load_data(year, ig)\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(year: int, ig: int) -> tuple[pd.DataFrame, pd.Index, pd.Index]:\n",
    "        if year == 2017:\n",
    "            ig_str = f\"LG {ig:02d}\"\n",
    "        else:\n",
    "            ig_str = f\"LG {ig:01d}\"\n",
    "\n",
    "        train = pd.read_csv(f\"tune/{year}_train.csv\")\n",
    "        val = pd.read_csv(f\"tune/{year}_val.csv\")\n",
    "        dataset = pd.concat([train, val], axis=0)\n",
    "\n",
    "        dataset = dataset[[\"Time stamp\", ig_str]]\n",
    "        dataset[ig_str] = dataset[ig_str].astype(float)\n",
    "        dataset.columns = [\"Time stamp\", \"target\"]\n",
    "        dataset[\"Time stamp\"] = pd.to_datetime(dataset.loc[:, \"Time stamp\"], errors='coerce')\n",
    "\n",
    "        train_idx = dataset[dataset[\"Time stamp\"].dt.month <= 8].index\n",
    "        val_idx = dataset[dataset[\"Time stamp\"].dt.month > 8].index\n",
    "\n",
    "        return dataset, train_idx, val_idx\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_public_holidays(year: int) -> list[datetime]:\n",
    "        with open(f\"holidays_{year}.json\", 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract unique dates and convert them to datetime objects\n",
    "        holiday_dates = set()\n",
    "        for state in data:\n",
    "            for holiday_info in data[state].values():\n",
    "                date_str = holiday_info[\"datum\"]\n",
    "                # holiday_dates.add(datetime.strptime(date_str, \"%Y-%m-%d\"))\n",
    "                holiday_dates.add(datetime.strptime(date_str, \"%Y-%m-%d\").date())\n",
    "\n",
    "        return list(holiday_dates)\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self):\n",
    "        raise\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, y_target):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self):\n",
    "        pass\n",
    "\n",
    "class TorchForecaster(Forecaster):\n",
    "    def __init__(\n",
    "            self,\n",
    "            year: int,\n",
    "            ig: int,\n",
    "            model: nn.Module,\n",
    "            loss_fn: nn.Module,\n",
    "            learning_rate: float = 0.001,\n",
    "            batch_size: int = 32,\n",
    "            peak_threshold: float = 0.85,\n",
    "            lookback: int = 672,\n",
    "            forecast_horizon: int = 48\n",
    "        ):\n",
    "        super().__init__(year, ig, lookback, forecast_horizon)\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.peak_threshold = peak_threshold\n",
    "\n",
    "        self.mean = self.dataset.loc[self.train_idx, \"target\"].mean()\n",
    "        self.std = self.dataset.loc[self.train_idx, \"target\"].std()\n",
    "        self.peak_value = self.dataset.loc[self.train_idx, \"target\"].max() * self.peak_threshold\n",
    "        self.normalized_peak_vaue = (self.peak_value - self.mean) / self.std\n",
    "\n",
    "        self.train_splitter = SlidingWindowSplitter(\n",
    "            fh=range(self.forecast_horizon + 1), \n",
    "            window_length=self.lookback,\n",
    "            step_length=1\n",
    "        )\n",
    "\n",
    "        self.val_splitter = SlidingWindowSplitter(\n",
    "            fh=range(self.forecast_horizon), \n",
    "            window_length=self.lookback,\n",
    "            step_length=1\n",
    "        )\n",
    "\n",
    "        self.holidays = self.get_public_holidays(self.year)\n",
    "\n",
    "        self.training_process = []\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def augment(self, train_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Augment the training data with additional features.\"\"\"\n",
    "\n",
    "        train_data.loc[:, \"target_normalized\"] = (train_data.loc[:, \"target\"] - self.mean) / self.std\n",
    "        train_data.loc[:, \"is_peak\"] = (train_data.loc[:, \"target_normalized\"] >= self.normalized_peak_vaue).astype(int)\n",
    "\n",
    "        train_data.loc[:, \"dow\"] = train_data[\"Time stamp\"].dt.dayofweek\n",
    "        train_data.loc[:, \"hour\"] = train_data[\"Time stamp\"].dt.hour\n",
    "\n",
    "        train_data.loc[:, \"is_holiday\"] = train_data[\"Time stamp\"].dt.date.isin(self.holidays).astype(int)\n",
    "\n",
    "        return train_data\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Data cleaning and normalization\"\"\"\n",
    "        self.dataset.loc[:, \"target\"] = self.dataset.loc[:, \"target\"].interpolate()\n",
    "        self.dataset = self.dataset.dropna(subset=[\"target\"])\n",
    "\n",
    "        self.dataset = self.augment(self.dataset)\n",
    "\n",
    "    def get_dataset(self, split: str = \"train\") -> TensorDataset:\n",
    "        print(f\"Creating {split} dataset...\")\n",
    "        if split == \"train\":\n",
    "            data = self.dataset[self.dataset[\"Time stamp\"].dt.month <= 8]\n",
    "            splitter = self.train_splitter\n",
    "        else:\n",
    "            data_train = self.dataset.iloc[-self.lookback:, :]\n",
    "            data_val = self.dataset.loc[self.dataset[\"Time stamp\"].dt.month > 8, :]\n",
    "            data = pd.concat([data_train, data_val], axis=0, ignore_index=True)\n",
    "            splitter = self.val_splitter\n",
    "\n",
    "        X_train_windows = []\n",
    "        y_train_windows = []\n",
    "\n",
    "        X_data = data[[\"target_normalized\", \"dow\", \"hour\", \"is_peak\", \"is_holiday\"]].values\n",
    "        y_data = data[[\"target_normalized\", \"is_peak\"]].values\n",
    "\n",
    "        for X_idx, y_idx in splitter.split(data[\"target_normalized\"]):\n",
    "            X_train, y_train = X_data[X_idx], y_data[y_idx]\n",
    "\n",
    "            X_train_windows.append(X_train)\n",
    "            y_train_windows.append(y_train)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train_windows, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train_windows, dtype=torch.float32)\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def train(self, epochs: int):\n",
    "        \"\"\"Fit the internal model(s)\"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        train_dataset = self.get_dataset(split=\"train\")\n",
    "        val_dataset = self.get_dataset(split=\"val\")\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        best_model_state_dict = None\n",
    "        best_loss = np.inf\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for i, (x_window, y_window) in enumerate(train_loader):\n",
    "                x_window = x_window.to(self.device)\n",
    "                y_window = y_window.to(self.device)\n",
    "\n",
    "                y_window_load = y_window[:, :, 0].unsqueeze(-1)\n",
    "                y_window_outlier = y_window[:, :, 1].unsqueeze(-1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred_window = self.model(x_window, y_window_load)\n",
    "                loss = combined_loss(\n",
    "                    y_true_load=y_window_load[:, 1:, :],\n",
    "                    y_pred_load=y_pred_window,\n",
    "                    y_true_outlier=y_window_outlier[:, 1:, :]\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += [loss.item()]\n",
    "            train_loss = np.mean(train_loss)\n",
    "\n",
    "            self.model.eval()\n",
    "            val_loss = []\n",
    "            with torch.no_grad():\n",
    "                for x_window, y_window in train_loader:\n",
    "                    x_window = x_window.to(self.device)\n",
    "                    y_window = y_window.to(self.device)\n",
    "\n",
    "                    y_window_load = y_window[:, :, 0].unsqueeze(-1)\n",
    "                    y_window_outlier = y_window[:, :, 1].unsqueeze(-1)\n",
    "\n",
    "                    y_pred_window = self.model(x_window, y_window_load, teacher_forcing_ratio=0.)\n",
    "                    loss = combined_loss(\n",
    "                        y_true_load=y_window_load[:, 1:, :],\n",
    "                        y_pred_load=y_pred_window,\n",
    "                        y_true_outlier=y_window_outlier[:, 1:, :]\n",
    "                    )\n",
    "                    val_loss += [loss.item()]\n",
    "            val_loss = np.mean(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model_state_dict = self.model.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}\")\n",
    "            self.training_process +=[{\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss\n",
    "            }]\n",
    "\n",
    "            assert best_model_state_dict is not None\n",
    "            self.model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        pd.DataFrame(self.training_process).to_csv(f\"training_process_{self.year}_{self.ig}.csv\", index=False)\n",
    "            \n",
    "    def predict(self, time_index: pd.Index) -> np.ndarray:\n",
    "        \"\"\"Predict the next value(s)\"\"\"\n",
    "        t_0 = time_index[0]\n",
    "\n",
    "        train_data = self.dataset.loc[self.dataset[\"Time stamp\"].dt < t_0, :]\n",
    "        train_data = train_data.iloc[-self.lookback, :]\n",
    "        x_window = train_data.loc[:, [\"target_normalized\", \"dow\", \"hour\", \"is_peak\", \"is_holiday\"]].values\n",
    "        \n",
    "        x_window = torch.tensor(x_window).to(self.device).unsqueeze(0)\n",
    "\n",
    "        # dummy\n",
    "        y_window_load = torch.zeros(1, self.forecast_horizon, 1).to(self.device)\n",
    "\n",
    "        y_pred_window = self.model(x_window, y_window_load, teacher_forcing_ratio=0.)\n",
    "\n",
    "        y_pred_window = y_pred_window.cpu().numpy()\n",
    "        \n",
    "        return y_pred_window\n",
    "\n",
    "    def update(self, y_target):\n",
    "        \"\"\"Update the model with the new value (if required)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model on the validation data\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # iterate over validation target using sliding window\n",
    "        for i_start in range(0, len(self.val_data), self.fh):\n",
    "            y_target = self.val_data.loc[i_start : i_start + self.fh - 1, \"target\"]\n",
    "\n",
    "\n",
    "            if len(y_target) < self.fh:\n",
    "                break\n",
    "\n",
    "            y_hat = self.predict()\n",
    "\n",
    "            # de-normalize\n",
    "            y_hat_denormalized = y_hat * self.std + self.mean\n",
    "            \n",
    "            # TODO plug in actual validation error metric\n",
    "            error = mean_absolute_percentage_error(y_target, y_hat_denormalized)\n",
    "\n",
    "            errors.append(error)\n",
    "\n",
    "            self.update(y_target)\n",
    "        \n",
    "        return np.mean(errors)\n",
    "\n",
    "input_dim = 5\n",
    "output_dim = 1  # For example, size of the output features (same as input for many-to-many)\n",
    "hidden_dim = 64  # LSTM hidden dimension\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "learning_rate = 0.001\n",
    "epochs = 2\n",
    "\n",
    "encoder = EncoderLSTM(input_dim, hidden_dim, num_layers).to(device)\n",
    "decoder = DecoderLSTM(output_dim, hidden_dim, num_layers).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "forecaster = TorchForecaster(\n",
    "    year=2017,\n",
    "    ig=1,\n",
    "    model=model,\n",
    "    learning_rate=learning_rate,\n",
    "    loss_fn=combined_loss,\n",
    "    lookback=672,\n",
    "    forecast_horizon=48\n",
    ")\n",
    "forecaster.preprocess()\n",
    "forecaster.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "training_process = pd.read_csv(\"training_process_2017_1.csv\")\n",
    "\n",
    "sns.lineplot(data=training_process, x=\"epoch\", y=\"train_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_fancy_forecaster(time_index_to_forecast, train_data):\n",
    "    return forecaster.predict(time_index_to_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_profiles_2016 = pd.read_csv('LoadProfile_20IPs_2016.csv', skiprows=1, delimiter=\";\", index_col=0, date_parser=custom_date_parser)\n",
    "actuals = pd.read_csv('tobi/2016_actuals.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "forecasts = actuals.copy()\n",
    "forecasts.loc[:, forecasts.columns != 'dataset_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_LENGTH = pd.Timedelta(hours=24*7)\n",
    "\n",
    "for dataset_id in actuals['dataset_id'].unique():\n",
    "    actuals_i = actuals[actuals['dataset_id'] == dataset_id]\n",
    "    for load in [x for x in actuals_i.columns if x != 'dataset_id']:\n",
    "        actuals_i_j = actuals_i[load]\n",
    "        start_of_test = actuals_i_j.index[0]\n",
    "        train_data_i_j = load_profiles_2016[load][start_of_test - TRAIN_DATA_LENGTH:start_of_test - pd.Timedelta(minutes=15)]\n",
    "        forecast = the_fancy_forecaster(actuals_i_j.index, train_data_i_j)\n",
    "        forecasts.loc[actuals_i_j.index, load] = forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(actual, predicted):\n",
    "    return np.sqrt(((actual - predicted) ** 2).mean())\n",
    "\n",
    "# Calculate RMSE for each pair of actual and predicted columns\n",
    "rmse_results = {}\n",
    "\n",
    "for col in [x for x in forecasts.columns if x != 'dataset_id']:\n",
    "    forecast = forecasts[col]\n",
    "    actual = actuals[col]\n",
    "    rmse_results[col] = calculate_rmse(actual, forecast)\n",
    "\n",
    "# Output RMSE for each column pair\n",
    "print(rmse_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
